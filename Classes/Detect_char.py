import cv2, json
import easyocr
import numpy as np
import re
import sys
from ultralytics import YOLO  # Th√™m d√≤ng n√†y ·ªü ƒë·∫ßu file
import Classes.Config as Config

# Preprocess functions
def preprocess(imgOriginal):
    GAUSSIAN_SMOOTH_FILTER_SIZE = (5, 5)   # K√≠ch c·ª° b·ªô l·ªçc Gauss
    ADAPTIVE_THRESH_BLOCK_SIZE = 19
    ADAPTIVE_THRESH_WEIGHT = 9

    def extractValue(imgOriginal):
        height, width, numChannels = imgOriginal.shape
        imgHSV = np.zeros((height, width, 3), np.uint8)
        imgHSV = cv2.cvtColor(imgOriginal, cv2.COLOR_BGR2HSV)
        imgHue, imgSaturation, imgValue = cv2.split(imgHSV)
        return imgValue

    def maximizeContrast(imgGrayscale):
        height, width = imgGrayscale.shape
        imgTopHat = np.zeros((height, width, 1), np.uint8)
        imgBlackHat = np.zeros((height, width, 1), np.uint8)
        structuringElement = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        imgTopHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_TOPHAT, structuringElement, iterations=10)
        imgBlackHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_BLACKHAT, structuringElement, iterations=10)
        imgGrayscalePlusTopHat = cv2.add(imgGrayscale, imgTopHat)
        imgGrayscalePlusTopHatMinusBlackHat = cv2.subtract(imgGrayscalePlusTopHat, imgBlackHat)
        return imgGrayscalePlusTopHatMinusBlackHat

    imgGrayscale = extractValue(imgOriginal)
    imgMaxContrastGrayscale = maximizeContrast(imgGrayscale)
    height, width = imgGrayscale.shape
    imgBlurred = cv2.GaussianBlur(imgMaxContrastGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)
    imgThresh = cv2.adaptiveThreshold(imgBlurred, 255.0, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, ADAPTIVE_THRESH_BLOCK_SIZE, ADAPTIVE_THRESH_WEIGHT)

    return imgGrayscale, imgThresh

# LicensePlateRecognizer class

class LicensePlateRecognizer:
    def __init__(self, languages=['en', 'vi']):
        self.reader = easyocr.Reader(languages)  # Kh·ªüi t·∫°o EasyOCR cho nh·∫≠n di·ªán vƒÉn b·∫£n
        # self.model = YOLO("D:\\DATN_nop_bai\\Model\\runs\\detect\\lp_yolov8n2\\weights\\best.pt")  # M√¥ h√¨nh YOLO
        self.model = YOLO(Config.MODEL_DETECT_LICENSE_PLATE)
        print("M√¥ h√¨nh YOLO ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
        
        # Load d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ JSON
        self.load_training_data("C:\\DATN_nop_bai\\Model\\training_data.json")

    def load_training_data(self, json_path):
        try:
            with open(json_path, 'r') as f:
                data = json.load(f)

                images = [item["image"] for item in data]
                labels = [ord(item["label"]) for item in data]  # ƒê·ªïi label th√†nh m√£ ASCII

                self.flattened_images = np.array(images, np.float32)
                self.classifications = np.array(labels, np.float32).reshape(-1, 1)

                print("‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ JSON.")
                print("üì¶ S·ªë l∆∞·ª£ng m·∫´u hu·∫•n luy·ªán:", len(labels))
        except Exception as e:
            print("‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu JSON:", e)
            self.classifications = None
            self.flattened_images = None

    def preprocess_image(self, img):
        imgGrayscale, imgThresh = preprocess(img)
        return imgThresh

    # L·ªçc v√† ch·ªâ gi·ªØ l·∫°i k√Ω t·ª± h·ª£p l·ªá nh∆∞ 68H-125.23
    def filter_text(self, text):
        text = text.upper()  # Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n th√†nh ch·ªØ hoa
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë, d·∫•u g·∫°ch ngang ho·∫∑c d·∫•u ch·∫•m
        text = re.sub(r'[^A-Z0-9\-\.]', '', text)
        
        # Ki·ªÉm tra xem c√≥ d·∫•u ch·∫•m hay kh√¥ng
        if '.' in text:
            # N·∫øu c√≥ d·∫•u ch·∫•m, ƒë·∫£m b·∫£o ch·ªâ c√≥ m·ªôt d·∫•u ch·∫•m
            text = re.sub(r'\.(?=.*\.)', '', text)
        return text

    def detect_license_plate_and_text(self, frame):
        if frame is None:
            print("·∫¢nh kh√¥ng h·ª£p l·ªá!")
            return frame, []

        # Nh·∫≠n di·ªán bi·ªÉn s·ªë v·ªõi m√¥ h√¨nh YOLO
        results = self.model(frame, conf=0.5, verbose=False)

        detected_texts = []
        for result in results:
            for box in result.boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                plate_img = frame[y1:y2, x1:x2]  # C·∫Øt bi·ªÉn s·ªë t·ª´ ·∫£nh

                h, w, _ = frame.shape
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)

                # ƒê·ªçc vƒÉn b·∫£n t·ª´ ·∫£nh bi·ªÉn s·ªë
                result_text = self.reader.readtext(plate_img)
                box_texts = []
                for detection in result_text:
                    text = self.filter_text(detection[1])  # L·ªçc vƒÉn b·∫£n
                    if text:  # N·∫øu c√≥ vƒÉn b·∫£n h·ª£p l·ªá
                        print("Bi·ªÉn s·ªë xe nh·∫≠n di·ªán ƒë∆∞·ª£c:", text)
                        box_texts.append(text)
                if box_texts:
                    # V·∫Ω h√¨nh ch·ªØ nh·∫≠t quanh bi·ªÉn s·ªë v√† hi·ªÉn th·ªã vƒÉn b·∫£n nh·∫≠n di·ªán
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    cv2.putText(frame, ' - '.join(box_texts), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    detected_texts.extend(box_texts) 

        # N·∫øu kh√¥ng c√≥ bi·ªÉn s·ªë n√†o ƒë∆∞·ª£c nh·∫≠n di·ªán, th√¥ng b√°o cho ng∆∞·ªùi d√πng
        if not detected_texts:
            print("Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c bi·ªÉn s·ªë xe.")

        return frame, detected_texts

    def process_image(self, image_path):
        img = cv2.imread(image_path)
        if img is None:
            print("Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh!")
            return None

        processed_img, detected_texts = self.detect_license_plate_and_text(img)
        output_path = 'output_license_plate.jpg'
        cv2.imwrite(output_path, processed_img)
        print(f"ƒê√£ l∆∞u ·∫£nh ƒë√£ nh·∫≠n di·ªán v√†o file: {output_path}")

        return detected_texts

    def show_live_detection(self):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("Kh√¥ng th·ªÉ m·ªü camera!")
            return

        while True:
            ret, frame = cap.read()
            if not ret:
                print("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera!")
                break

            processed_frame, detected_texts = self.detect_license_plate_and_text(frame)
            cv2.imshow("Live License Plate Detection", processed_frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('s'):
                cv2.imwrite("output_live_license_plate.jpg", processed_frame)
                print("ƒê√£ l∆∞u ·∫£nh ƒë√£ nh·∫≠n di·ªán v√†o file: output_live_license_plate.jpg")
            elif key == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()